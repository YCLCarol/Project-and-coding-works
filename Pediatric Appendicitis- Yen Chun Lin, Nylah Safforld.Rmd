---
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
  html_document: default
---
\subsection{Exploratory Data Analysis: Pediatric Appendicitis}

**Reading data into R**
```{r}
library(tidyverse)

append_data <- read_csv("app_data.csv")
head(append_data)

```
**Data length - Original**
```{r}
nrow(append_data)
```

**Number of Variables - Original**
```{r}
ncol(append_data)
```

**Removing NA values**

There was many NA values in the dataset, some columns with over 50% missing values.


```{r}
threshold <- 0.5
reduced <- append_data %>%
  select_if(~ mean(is.na(.)) <= threshold)
```

```{r,message=F,warning=F}
data_mutate <- reduced %>%
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```
**Clean Data**
```{r}
row_threshold <- 1
clean_data <- data_mutate %>%
  filter(rowSums(is.na(.)) <= row_threshold)

clean_data
```

- Remove NA values.


```{r}
library(car)

clean_data <- clean_data %>%
  mutate(across(where(is.character), as.factor))

clean_data

```

- Convert the variables that are of the character type to factors.

```{r}
unique(clean_data$Ketones_in_Urine)
unique(clean_data$RBC_in_Urine)
unique(clean_data$WBC_in_Urine)
```
```{r}
clean_data <- clean_data[, !names(clean_data) %in% c("Ketones_in_Urine", "RBC_in_Urine", "WBC_in_Urine", "Free_Fluids")]

```

Remove any remaining variables with na
```{r}
clean_data <- na.omit(clean_data)
clean_data
```
```{r}

library(car)

# Fit a full linear regression model
full_model <- glm(Diagnosis ~ ., family = "binomial", data = clean_data)

summary(full_model)

# Calculate VIF for each predictor in the model
vif_values <- vif(full_model)
avg_gvif <- mean(vif_values)
avg_gvif


```



Perform Subset Selection 

AIC
```{r}

library(car)  

full_model <- suppressWarnings(glm(Diagnosis ~ ., family = "binomial", data = clean_data))
summary(full_model)

vif_values <- suppressWarnings(vif(full_model))
print(vif_values)

AIC_model <- suppressWarnings(step(full_model, direction = "both"))
summary(AIC_model)

```

- The best AIC model contains 11 predictors. 
 

BIC
```{r}

BIC_model <- suppressWarnings(step(full_model, direction = "both", k = log(length(full_model$fitted.values))))
summary(BIC_model)
```

- The best BIC model has 4 predictors.

- AIC or BIC: AIC will be used due to the lower score.  

```{r}
AIC_final_model<- glm(Diagnosis ~ Weight + Management + Alvarado_Score + 
    Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
    Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia, 
    family = "binomial", data = clean_data)

vif(AIC_final_model)
summary(AIC_final_model)
```

```{r}
BIC_final_model <- glm(Diagnosis ~ Management + Alvarado_Score + Appendix_on_US + 
    Appendix_Diameter, family = "binomial", data = clean_data)
vif(BIC_final_model)
summary(BIC_final_model)
```


**Exploratory Data Analysis**


**Data length - Clean Data**
```{r}
nrow(clean_data)
```

**Number of Variables - Clean Data**
```{r}
ncol(clean_data)
```

**Summary of the Data**
```{r}
summary(clean_data)
```
**Data Structure**
```{r}
str(clean_data)
```
**Response Variable "Diagnosis" Distribution**
```{r}
table(clean_data$Diagnosis)
```

**Checking out the variables**

```{r,message=FALSE,warning=FALSE}
library(ggplot2)
ggplot(clean_data, aes(x = Age)) + geom_histogram(binwidth = 1, color = "black") +
  ggtitle("Distribution of Age")
```

- The ages range from around 0 to 18.

- A slightly left skewed distribution

```{r}
ggplot(clean_data, aes(x = BMI)) + geom_histogram(binwidth = 2, color = "black") +
  ggtitle("Distribution of BMI")
```

- Ranges from a BMI of 0 to 40.

- A slightly right skewed distribution.

```{r}
ggplot(clean_data, aes(x = Diagnosis, y = Age)) + geom_boxplot() + ggtitle(" Age by Diagnosis")
```

- The median age and spread of ages are similar in both groups.

```{r,message=FALSE,warning=FALSE}
ggplot(clean_data, aes(x = Diagnosis, y = BMI)) + geom_boxplot() + ggtitle("BMI by Diagnosis")
```

- The median BMI and spread of BMI are similar in both groups.

**Correlation Matrix**
```{r}

columns <- c("Weight" , "Alvarado_Score" , "Appendix_Diameter" ,"Body_Temperature" ,"WBC_Count" , "Thrombocyte_Count")

corr_vars <- clean_data[, columns]


cor_matrix <- cor(corr_vars, use = "complete.obs", method = "sp")

print(cor_matrix)

help("methods")

```

```{r}
library(corrplot)
corrplot(cor_matrix, method = "circle")

```

- Most variables have weak or very weak correlations.

- The Alvarado Score shows a moderate correlation with both WBC count (white blood cell count) and body temperature. 

**Chi-Square Test**

Management
```{r}
chisq_test_results <- chisq.test(table(clean_data$Diagnosis, clean_data$Management))
print(chisq_test_results)

```

- The p-value (< 2.2e-16) is lower than the alpha value (0.05), indicating a significant association between the variables Diagnosis and Management.

Coughing
```{r}
chisq_test_results <- chisq.test(table(clean_data$Diagnosis, clean_data$Coughing_Pain))
print(chisq_test_results)
```

- The p-value (0.009185) is lower than the alpha value (0.05), indicating a significant association between the variables Diagnosis and Coughing Pain.

Ultrasound on Appendix
```{r}
chisq_test_results <- chisq.test(table(clean_data$Diagnosis, clean_data$Appendix_on_US))
print(chisq_test_results)
```

- The p-value (<2.2e-16) is lower than the alpha value (0.05), indicating a significant association between the variables Diagnosis and Ultrasound on Appendix. 

Nausea
```{r}
chisq_test_results <- chisq.test(table(clean_data$Diagnosis, clean_data$Nausea))
print(chisq_test_results)

```

- The p-value (0.0007367) is lower than the alpha value (0.05), indicating a significant association between the variables Diagnosis and Nausea.

Neutrophilia
```{r}
chisq_test_results <- chisq.test(table(clean_data$Diagnosis, clean_data$Neutrophilia))
print(chisq_test_results)

```

- The p-value (5.521e-05) is lower than the alpha value (0.05), indicating a significant association between the variables Diagnosis and Neutrophilia.

KNN Model
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(class)

# Normalize numeric columns
numeric_columns <- clean_data %>% 
  select(Weight, Alvarado_Score, Appendix_Diameter, Body_Temperature, WBC_Count, Thrombocyte_Count, Neutrophilia)

preProcess_values <- preProcess(numeric_columns, method = c("center", "scale"))
clean_data_normalized <- predict(preProcess_values, clean_data)

# Combine normalized numeric columns with categorical columns
clean_data_normalized <- cbind(
  clean_data_normalized[, c("Weight", "Alvarado_Score", "Appendix_Diameter", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia")],
  clean_data[, c("Management", "Appendix_on_US", "Coughing_Pain", "Nausea", "Diagnosis")]
)

# Select relevant columns
selected_columns <- c("Weight", "Management", "Alvarado_Score", "Appendix_on_US", "Appendix_Diameter", "Coughing_Pain", 
                      "Nausea", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia", "Diagnosis")
clean_data_normalized <- clean_data_normalized[, selected_columns] 

# Remove rows with missing values
clean_data_normalized <- na.omit(clean_data_normalized)

# Split data into training and testing sets
set.seed(2024)
train_index <- createDataPartition(clean_data_normalized$Diagnosis, p = 0.8, list = FALSE)
auto.training <- clean_data_normalized[train_index, ]
auto.testing <- clean_data_normalized[-train_index, ]

X.train <- auto.training[, -ncol(auto.training)]
Y.train <- auto.training$Diagnosis
X.test <- auto.testing[, -ncol(auto.testing)]  
Y.test <- auto.testing$Diagnosis


X.train <- model.matrix(~ . -1, data = X.train)
X.test <- model.matrix(~ . -1, data = X.test) 

# Fit KNN model 
set.seed(2024)
append.knn <- knn(train = X.train, test = X.test, cl = Y.train, k = 6)

# Confusion matrix and accuracy 
conf_matrix <- table(Y.test, append.knn)
print(conf_matrix)
test_accuracy <- mean(Y.test == append.knn)
print(paste("Test accuracy with k=6:", test_accuracy))

# optimal value of K
Kmax <- 100
class.rate.test <- rep(0, Kmax)
class.rate.train <- rep(0, Kmax)

for (i in 1:Kmax) {
  knn.test <- knn(train = X.train, test = X.test, cl = Y.train, k = i)
  knn.train <- knn(train = X.train, test = X.train, cl = Y.train, k = i)
  class.rate.test[i] <- mean(Y.test == knn.test)
  class.rate.train[i] <- mean(Y.train == knn.train)
}

# Plot classification rates 
plot(c(1:Kmax), class.rate.test, type = "p", col = "red", xlab = "K", ylab = "Classification rate", ylim = c(0, 1))
points(c(1:Kmax), class.rate.train, col = "blue")
legend("bottomright", legend = c("Test", "Train"), col = c("red", "blue"), pch = 1)

# refit the model with optimal k 
k.opt <- which.max(class.rate.test)
print(c(k.opt, class.rate.test[which.max(class.rate.test)]))

# Fit KNN model with the optimal k
append.knnOpt <- knn(train = X.train, test = X.test, cl = Y.train, k = k.opt)

# Confusion matrix and accuracy  - testing 
conf_matrix_opt <- table(Y.test, append.knnOpt)
print(conf_matrix_opt)
test_accuracy_opt <- mean(Y.test == append.knnOpt)
print(paste("Test accuracy with optimal k:", test_accuracy_opt))

# Confusion matrix and accuracy - training 
train_knn_opt <- knn(train = X.train, test = X.train, cl = Y.train, k = k.opt)
conf_matrix_train_opt <- table(Y.train, train_knn_opt)
print(conf_matrix_train_opt)
train_accuracy_opt <- mean(Y.train == train_knn_opt)
print(paste("Train accuracy with optimal k:", train_accuracy_opt))




```

KNN Model with Cross Validation 

```{r}

library(dplyr)
library(caret)
library(class)

# Normalize numeric columns
numeric_columns <- clean_data %>% 
  select(Weight, Alvarado_Score, Appendix_Diameter, Body_Temperature, WBC_Count, Thrombocyte_Count, Neutrophilia)

preProcess_values <- preProcess(numeric_columns, method = c("center", "scale"))
clean_data_normalized <- predict(preProcess_values, clean_data)

# Combine normalized numeric columns with categorical columns
clean_data_normalized <- cbind(
  clean_data_normalized[, c("Weight", "Alvarado_Score", "Appendix_Diameter", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia")],
  clean_data[, c("Management", "Appendix_on_US", "Coughing_Pain", "Nausea", "Diagnosis")]
)

# Select relevant columns
selected_columns <- c("Weight", "Management", "Alvarado_Score", "Appendix_on_US", "Appendix_Diameter", "Coughing_Pain", 
                      "Nausea", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia", "Diagnosis")
clean_data_normalized <- clean_data_normalized[, selected_columns] 

# Remove rows with missing values
clean_data_normalized <- na.omit(clean_data_normalized)

# Split data into training and testing sets
set.seed(2024)
train_index <- createDataPartition(clean_data_normalized$Diagnosis, p = 0.8, list = FALSE)
auto.training <- clean_data_normalized[train_index, ]
auto.testing <- clean_data_normalized[-train_index, ]

X.train <- auto.training[, -ncol(auto.training)]
Y.train <- auto.training$Diagnosis
X.test <- auto.testing[, -ncol(auto.testing)]  
Y.test <- auto.testing$Diagnosis

# Matrix form
X.train <- model.matrix(~ . -1, data = X.train)
X.test <- model.matrix(~ . -1, data = X.test) 

# Cross-validation to find the optimal k
set.seed(2024)
train_control <- trainControl(method = "cv", number = 10)
knn_fit <- train(x = X.train, y = Y.train, method = "knn", trControl = train_control, tuneLength = 10)

#best tuning parameter k
print(knn_fit$bestTune)

# Fit KNN model
optimal_k <- knn_fit$bestTune$k
append.knnOpt <- knn(train = X.train, test = X.test, cl = Y.train, k = optimal_k)

# Confusion matrix and accuracy 
conf_matrix_opt <- table(Y.test, append.knnOpt)
print(conf_matrix_opt)
test_accuracy_opt <- mean(Y.test == append.knnOpt)
print(paste("Test accuracy with optimal k:", test_accuracy_opt))

# Confusion matrix and accuracy 
train_knn_opt <- knn(train = X.train, test = X.train, cl = Y.train, k = optimal_k)
conf_matrix_train_opt <- table(Y.train, train_knn_opt)
print(conf_matrix_train_opt)
train_accuracy_opt <- mean(Y.train == train_knn_opt)
print(paste("Train accuracy with optimal k:", train_accuracy_opt))

#classification rate for different values of K
Kmax <- 100
class.rate.test <- rep(0, Kmax)
class.rate.train <- rep(0, Kmax)

for (i in 1:Kmax) {
  knn.test <- knn(train = X.train, test = X.test, cl = Y.train, k = i)
  knn.train <- knn(train = X.train, test = X.train, cl = Y.train, k = i)
  class.rate.test[i] <- mean(Y.test == knn.test)
  class.rate.train[i] <- mean(Y.train == knn.train)
}

# Plot classification rates 
plot(c(1:Kmax), class.rate.test, type = "p", col = "red", xlab = "K", ylab = "Classification rate", ylim = c(0, 1))
points(c(1:Kmax), class.rate.train, col = "blue")
legend("bottomright", legend = c("Test", "Train"), col = c("red", "blue"), pch = 1)

# Print the optimal K 
print(paste("Optimal K from cross-validation:", optimal_k))

total_conf_matrix <- table(c(Y.train, Y.test), c(train_knn_opt, append.knnOpt))
overall_accuracy <- mean(c(Y.train, Y.test) == c(train_knn_opt, append.knnOpt))
print(total_conf_matrix)
print(paste("Overall accuracy with optimal k:", overall_accuracy))



```
Holdout Data - KNN

```{r}
library(dplyr)
library(caret)
library(class)

# Normalize numeric columns
numeric_columns <- clean_data %>% 
  select(Weight, Alvarado_Score, Appendix_Diameter, Body_Temperature, WBC_Count, Thrombocyte_Count, Neutrophilia)

preProcess_values <- preProcess(numeric_columns, method = c("center", "scale"))
clean_data_normalized <- predict(preProcess_values, clean_data)

# Combine normalized numeric columns with categorical columns
clean_data_normalized <- cbind(
  clean_data_normalized[, c("Weight", "Alvarado_Score", "Appendix_Diameter", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia")],
  clean_data[, c("Management", "Appendix_on_US", "Coughing_Pain", "Nausea", "Diagnosis")]
)

# Select relevant columns
selected_columns <- c("Weight", "Management", "Alvarado_Score", "Appendix_on_US", "Appendix_Diameter", "Coughing_Pain", 
                      "Nausea", "Body_Temperature", "WBC_Count", "Thrombocyte_Count", "Neutrophilia", "Diagnosis")
clean_data_normalized <- clean_data_normalized[, selected_columns] 

# Remove rows with missing values
clean_data_normalized <- na.omit(clean_data_normalized)

# Split data 
set.seed(2024)
holdout_index <- createDataPartition(clean_data_normalized$Diagnosis, p = 0.2, list = FALSE)
holdout_data <- clean_data_normalized[holdout_index, ]
training_validation_data <- clean_data_normalized[-holdout_index, ]

# Split data
train_index <- createDataPartition(training_validation_data$Diagnosis, p = 0.8, list = FALSE)
auto.training <- training_validation_data[train_index, ]
auto.validation <- training_validation_data[-train_index, ]

X.train <- auto.training[, -ncol(auto.training)]
Y.train <- auto.training$Diagnosis
X.validation <- auto.validation[, -ncol(auto.validation)]  
Y.validation <- auto.validation$Diagnosis
X.holdout <- holdout_data[, -ncol(holdout_data)]
Y.holdout <- holdout_data$Diagnosis

#Matrix form
X.train <- model.matrix(~ . -1, data = X.train)
X.validation <- model.matrix(~ . -1, data = X.validation)
X.holdout <- model.matrix(~ . -1, data = X.holdout)

# Fit KNN model
set.seed(2024)
optimal_k <- 6  # Set the k value as per your previous model without cross-validation
append.knn <- knn(train = X.train, test = X.validation, cl = Y.train, k = optimal_k)

# Confusion matrix 
conf_matrix_validation <- table(Y.validation, append.knn)
print(conf_matrix_validation)
validation_accuracy <- mean(Y.validation == append.knn)
print(paste("Validation accuracy with k =", optimal_k, ":", validation_accuracy))

# Confusion matrix 
train_knn <- knn(train = X.train, test = X.train, cl = Y.train, k = optimal_k)
conf_matrix_train <- table(Y.train, train_knn)
print(conf_matrix_train)
train_accuracy <- mean(Y.train == train_knn)
print(paste("Train accuracy with k =", optimal_k, ":", train_accuracy))

# Classification rate 
Kmax <- 100
class.rate.train_validation <- rep(0, Kmax)
class.rate.validation <- rep(0, Kmax)

for (i in 1:Kmax) {
  knn.train_validation <- knn(train = X.train, test = X.train, cl = Y.train, k = i)
  knn.validation <- knn(train = X.train, test = X.validation, cl = Y.train, k = i)
  class.rate.train_validation[i] <- mean(Y.train == knn.train_validation)
  class.rate.validation[i] <- mean(Y.validation == knn.validation)
}

#Plot for Classification rates
plot(c(1:Kmax), class.rate.train_validation, type = "p", col = "blue", xlab = "K", ylab = "Classification rate", ylim = c(0, 1))
points(c(1:Kmax), class.rate.validation, col = "green")
legend("bottomright", legend = c("Train+Validation", "Validation"), col = c("blue", "green"), pch = 1)

# Evaluate  model on holdout data
append.knnOpt_holdout <- knn(train = X.train, test = X.holdout, cl = Y.train, k = optimal_k)

# Confusion matrix +  accuracy for holdout data (optimal k)
conf_matrix_holdout <- table(Y.holdout, append.knnOpt_holdout)
print(conf_matrix_holdout)
holdout_accuracy_opt <- mean(Y.holdout == append.knnOpt_holdout)
print(paste("Holdout accuracy with optimal k:", holdout_accuracy_opt))

#Accuracy with holdout data
overall_accuracy <- mean(Y.holdout == append.knnOpt_holdout)
print(paste("Overall accuracy with holdout data:", overall_accuracy))

```

- Using AIC model.

- First, set K=6, the accuracy was 83.1%

- Tuned model for the optimal K

- The optimal K=26 with a classification rate of 94.38%

- Training accuracy:89.89%

- Testing accuracy: 88.86

- Cross validation model: ~92%

- The classification rate with holdout data: 90%

- Performance using holdout data shows an accuracy rate of 90%. Although it still performs well, it performs worse than the model with the optimal K.




**Logistic Regression**
```{r,message=FALSE,warning=FALSE}
AIC_final_model<- glm(Diagnosis ~ Weight + Management + Alvarado_Score + 
    Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
    Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia, 
    family = "binomial", data = clean_data)
Probability <- predict(AIC_final_model, auto.testing, type ="response")
Predicted.Diagnosis <- ifelse(Probability >= .5, "appendicitis", "no appendicitis")
table(auto.testing$Diagnosis, Predicted.Diagnosis)
mean(auto.testing$Diagnosis == Predicted.Diagnosis)
```

Holdout Data - Logistic Regression
```{r}
library(dplyr)
library(caret)

# Split data 
set.seed(2024)
holdout_index <- createDataPartition(clean_data$Diagnosis, p = 0.2, list = FALSE)
holdout_data <- clean_data[holdout_index, ]
training_validation_data <- clean_data[-holdout_index, ]

# Fit logistic regression model 
AIC_final_model <- glm(Diagnosis ~ Weight + Management + Alvarado_Score + 
                         Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
                         Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia, 
                       family = "binomial", data = training_validation_data)

# Evaluate model on holdout data
Probability_holdout <- predict(AIC_final_model, holdout_data, type ="response")
Predicted.Diagnosis_holdout <- ifelse(Probability_holdout >= .5, "appendicitis", "no appendicitis")

# Confusion matrix 
conf_matrix_holdout <- table(holdout_data$Diagnosis, Predicted.Diagnosis_holdout)
print(conf_matrix_holdout)
holdout_accuracy <- mean(holdout_data$Diagnosis == Predicted.Diagnosis_holdout)
print(paste("Holdout accuracy:", holdout_accuracy))

# Holdout Data Accuracy 
true_positives <- conf_matrix_holdout["appendicitis", "appendicitis"]
true_negatives <- conf_matrix_holdout["no appendicitis", "no appendicitis"]
total_correct <- true_positives + true_negatives
total_holdout <- sum(conf_matrix_holdout)
overall_accuracy_holdout <- total_correct / total_holdout
print(paste("Overall accuracy with holdout data:", overall_accuracy_holdout))

# Confusion matrix 
Probability_training_validation <- predict(AIC_final_model, training_validation_data, type ="response")
Predicted.Diagnosis_training_validation <- ifelse(Probability_training_validation >= .5, "appendicitis", "no appendicitis")

conf_matrix_training_validation <- table(training_validation_data$Diagnosis, Predicted.Diagnosis_training_validation)
print(conf_matrix_training_validation)
training_validation_accuracy <- mean(training_validation_data$Diagnosis == Predicted.Diagnosis_training_validation)
print(paste("Training+validation accuracy:", training_validation_accuracy))

# Accuracy 
true_positives_train <- conf_matrix_training_validation["appendicitis", "appendicitis"]
true_negatives_train <- conf_matrix_training_validation["no appendicitis", "no appendicitis"]
total_correct_train <- true_positives_train + true_negatives_train
total_train <- sum(conf_matrix_training_validation)
overall_accuracy_training_validation <- total_correct_train / total_train
print(paste("Overall accuracy with training+validation data:", overall_accuracy_training_validation))


```

- Using AIC model.

- Classification rate: 21.35%

- The classification rate of the logistic regression model is much lower than that of the KNN model.

- The classification rate with holdout data: 7% 

- The logistic regression model performs significantly worse on holdout data, with a 7% accuracy compared to 21% for the initial model.




**LDA**
```{r}
library(MASS)
lda.fit<- lda(Diagnosis ~ Weight + Management + Alvarado_Score + 
    Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
    Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia,data = clean_data)
Predicted.Diagonsis_lda <- predict(lda.fit, clean_data)$class
table(clean_data$Diagnosis, Predicted.Diagonsis_lda)
mean(clean_data$Diagnosis==Predicted.Diagonsis_lda)
```

LDA with Holdout Data
```{r}
library(MASS)
library(caret)

# Split data 
set.seed(2024)
holdout_index <- createDataPartition(clean_data$Diagnosis, p = 0.2, list = FALSE)
holdout_data <- clean_data[holdout_index, ]
training_validation_data <- clean_data[-holdout_index, ]

# Fit LDA model 
lda.fit <- lda(Diagnosis ~ Weight + Management + Alvarado_Score + 
                 Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
                 Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia, 
               data = training_validation_data)

# Evaluate LDA model on Holdout
Predicted.Diagonsis_lda_holdout <- predict(lda.fit, holdout_data)$class
conf_matrix_lda_holdout <- table(holdout_data$Diagnosis, Predicted.Diagonsis_lda_holdout)
print(conf_matrix_lda_holdout)
holdout_accuracy_lda <- mean(holdout_data$Diagnosis == Predicted.Diagonsis_lda_holdout)
print(paste("LDA Holdout accuracy:", holdout_accuracy_lda))

# Accuracy

true_positives_lda <- conf_matrix_lda_holdout["appendicitis", "appendicitis"]
true_negatives_lda <- conf_matrix_lda_holdout["no appendicitis", "no appendicitis"]
total_correct_lda <- true_positives_lda + true_negatives_lda
total_holdout_lda <- sum(conf_matrix_lda_holdout)
overall_accuracy_holdout_lda <- total_correct_lda / total_holdout_lda
print(paste("Overall accuracy with holdout data for LDA:", overall_accuracy_holdout_lda))

# Evaluate LDA model 
Predicted.Diagonsis_lda_train <- predict(lda.fit, training_validation_data)$class
conf_matrix_lda_train <- table(training_validation_data$Diagnosis, Predicted.Diagonsis_lda_train)
print(conf_matrix_lda_train)
training_accuracy_lda <- mean(training_validation_data$Diagnosis == Predicted.Diagonsis_lda_train)
print(paste("LDA Training+validation accuracy:", training_accuracy_lda))
```

- Using AIC model.

- Classification rate: 90.84%

- The classification rate with holdout data: 91%

- The LDA model performs better on the holdout data than on the initial data by about 1%.


**QDA**
```{r,message=FALSE,warning=FALSE}
library(MASS)
logit_model <- glm(Diagnosis ~ Weight + Management + Alvarado_Score + 
                     Appendix_on_US + Appendix_Diameter + Coughing_Pain + Nausea + 
                     Body_Temperature + WBC_Count + Thrombocyte_Count + Neutrophilia,
                   data = clean_data, family = binomial)
qda_fit <- qda(Diagnosis ~ Weight + Alvarado_Score + 
                 Coughing_Pain + Nausea + Body_Temperature,
               data = clean_data)


Predicted.Diagonsis_qda <- predict(qda_fit, clean_data)$class

table(clean_data$Diagnosis, Predicted.Diagonsis_qda)
mean(clean_data$Diagnosis==Predicted.Diagonsis_qda)
```
QDA with Holdout Data

```{r}
library(MASS)
library(caret)

# Split data
set.seed(2024)
holdout_index <- createDataPartition(clean_data$Diagnosis, p = 0.2, list = FALSE)
holdout_data <- clean_data[holdout_index, ]
training_validation_data <- clean_data[-holdout_index, ]

# Fit QDA model
qda_fit <- qda(Diagnosis ~ Weight + Alvarado_Score + 
                 Coughing_Pain + Nausea + Body_Temperature,
               data = training_validation_data)

# Evaluate QDA model on holdout data
Predicted.Diagonsis_qda_holdout <- predict(qda_fit, holdout_data)$class
conf_matrix_qda_holdout <- table(holdout_data$Diagnosis, Predicted.Diagonsis_qda_holdout)
print(conf_matrix_qda_holdout)
holdout_accuracy_qda <- mean(holdout_data$Diagnosis == Predicted.Diagonsis_qda_holdout)
print(paste("QDA Holdout accuracy:", holdout_accuracy_qda))

#accuracy 
true_positives_qda <- conf_matrix_qda_holdout["appendicitis", "appendicitis"]
true_negatives_qda <- conf_matrix_qda_holdout["no appendicitis", "no appendicitis"]
total_correct_qda <- true_positives_qda + true_negatives_qda
total_holdout_qda <- sum(conf_matrix_qda_holdout)
overall_accuracy_holdout_qda <- total_correct_qda / total_holdout_qda
print(paste("Overall accuracy with holdout data (QDA):", overall_accuracy_holdout_qda))

# Evaluate QDA model
Predicted.Diagonsis_qda_train <- predict(qda_fit, training_validation_data)$class
conf_matrix_qda_train <- table(training_validation_data$Diagnosis, Predicted.Diagonsis_qda_train)
print(conf_matrix_qda_train)
training_accuracy_qda <- mean(training_validation_data$Diagnosis == Predicted.Diagonsis_qda_train)
print(paste("QDA Training+validation accuracy:", training_accuracy_qda))

```


- Using AIC model.

- Classification rate: 65.63%

- The classification rate with holdout data: 67%

- The QDA model performs better on the holdout data than on the initial data by about 1%

Summary: 

- Since the KNN model has the highest correct prediction rate, we recommend using KNN for this dataset.
